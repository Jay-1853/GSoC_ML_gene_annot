# GSoC_ML_gene_annot
GSoC HAVANA project "Using machine learning to annotate difficult genes"

## Brief Explanation

Understanding the impact of genetic variation on disease requires comprehensive gene annotation. Human genes are well characterised following more than two decades of work on their annotation, however, we know that this annotation is not complete and that new experimental methods are generating data to help us towards the goal of complete gene annotation. 

We have developed an automated workflow to use long transcriptomic data to add novel alternatively spliced transcripts to our gene annotation. Our method uses very strict thresholds to ensure that no poor-quality models are added to the gene annotation, although as a consequence we reject significant numbers of viable novel transcripts. We want to use machine learning to recover good quality but rejected transcripts and improve the setting of initial filters for new datasets.

### Expected results

- Install and learn to use a machine learning package
- Run it on known gene annotation
- Deliverable: simple model that helps to recover valid transcripts; set of most relevant features for decision making

### Required knowledge

Machine learning

### Difficulty

Medium

### Mentors

Jonathan Mudge, Jose Manuel Gonzalez Martinez, Adam Frankish



## Detailed description of the project
We have developed an automated workflow to use long transcriptomic data to add novel alternatively spliced transcripts to our gene annotation.

- In principal the method is agnostic to the method used to generate the annotation as further filters are applied in addition to those in the primary annotation pipeline but in practice the better the primary annotation method, the better the results

- Our manual annotation is higher quality than that generated by any purely computational method so we need to set our standards very high and only allow high quality transcripts be added automatically to the annotation

  - In practice we would look for an error rate of <0.1%


Our method uses very strict thresholds to ensure that no poor-quality models are added to the gene annotation

- Filters were derived from manual QC of thousands of transcripts

- Filters include:

  - using a very limited definition of canonical splicing - GT-AG only

    - Any other introns rejected

  - using the expression of any novel introns relative to the expression of annotated introns at the same locus

    - Using Recount 3 RNAseq intron spanning reads (https://rna.recount.bio)

  - using overlaps of splice sites with repeat elements

    - Transposable elements and tandem repeats

  - using overlap on the opposite strand with annotated transcripts
  - using an minimum intron length threshold of 50 nt

- All of these filters capture a large number of valid transcript models

  - Estimated false negative rate ~20%

  - So there are large numbers of valid transcripts being rejected


We want to use machine learning to recover good quality but rejected transcripts and improve the setting of initial filters for new datasets

- We have a large set of high confidence introns in the current annotation

- BUT this is subject to bias - particularly as they are expressed at a higher level

- Manual investigation suggests that Recount3 data will be important to help discriminate rejected introns but cannot be relied on absolutely


We would like the project to investigate other potential data that could be used to reduce the false negative rate for annotation filtering

- Eg features not investigated to date include genome sequence beyond the core splice site, splice site strength and evolutionary conservation of splice sites

- Any validated reduction would be considered successful

- Billions of reads will be processed over the next few years so any improvement in the recovery of transcripts could lead to the incorporation of thousands of transcripts into the annotation


Mentors will be able to offer guidance on the use of existing annotation, the biology of the processes we are trying to model in the data, finding published data to support the work of the project and the identification of training/test sets.
